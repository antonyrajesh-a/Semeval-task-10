{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "3HfZLlQOnjUX"
      },
      "outputs": [],
      "source": [
        "train_path = \"../data/MaSaC_train_efr.json\"\n",
        "test_path = \"../data/MaSaC_test_efr.json\"\n",
        "\n",
        "test_labels='../data/MaSaC_test_efr_labels.json'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y3miGcCHG5xo"
      },
      "source": [
        "XGB CLASSIFIER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DoxEwIy89-vF",
        "outputId": "e4dd2f9e-4b8d-4eeb-eed5-d2a9c19347d0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "F1 Score: 0.7915106117353308\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import numpy as np\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "\n",
        "def read_json(path):\n",
        "    train_list_ = []\n",
        "    label_list_ = []\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as file_read:\n",
        "        data_line = json.loads(file_read.read())\n",
        "\n",
        "\n",
        "        for idx, line_i in enumerate(data_line):\n",
        "\n",
        "            for idx_i, speaker_i, utterances_i, emotion_i, trigger_i in zip(range(len(line_i['speakers'])),\n",
        "                                                                            line_i['speakers'],\n",
        "                                                                            line_i['utterances'],\n",
        "                                                                            line_i['emotions'], line_i['triggers']):\n",
        "                if idx_i == len(line_i['speakers']) - 1:\n",
        "                    line_emo = [label_name.index(emotion_i), label_name.index(first_emotion),\n",
        "                                label_name.index(first_emotion), label_name.index(first_emotion)]\n",
        "                elif idx_i == len(line_i['speakers']) - 2:\n",
        "                    line_emo = [label_name.index(emotion_i), label_name.index(line_i['emotions'][idx_i + 1]),\n",
        "                                label_name.index(first_emotion), label_name.index(first_emotion)]\n",
        "                elif idx_i == len(line_i['speakers']) - 3:\n",
        "                    line_emo = [label_name.index(emotion_i), label_name.index(line_i['emotions'][idx_i + 1]),\n",
        "                                label_name.index(line_i['emotions'][idx_i + 2]), label_name.index(first_emotion)]\n",
        "                else:\n",
        "                    line_emo = [label_name.index(emotion_i), label_name.index(line_i['emotions'][idx_i + 1]),\n",
        "                                label_name.index(line_i['emotions'][idx_i + 2]),\n",
        "                                label_name.index(line_i['emotions'][idx_i + 3])]\n",
        "\n",
        "                try:\n",
        "                    label_list_.append(int(trigger_i))\n",
        "\n",
        "                    train_list_.append(line_emo)\n",
        "\n",
        "\n",
        "                except Exception as e:\n",
        "                    pass\n",
        "\n",
        "\n",
        "\n",
        "    return np.array(train_list_), np.array(label_list_)\n",
        "\n",
        "\n",
        "def read_json_test(path):\n",
        "    train_list_ = []\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as file_read:\n",
        "        data_line = json.loads(file_read.read())\n",
        "\n",
        "        for idx, line_i in enumerate(data_line):\n",
        "\n",
        "\n",
        "            for idx_i, speaker_i, utterances_i, emotion_i in zip(range(len(line_i['speakers'])),\n",
        "                                                                 line_i['speakers'],\n",
        "                                                                 line_i['utterances'],\n",
        "                                                                 line_i['emotions']):\n",
        "                if idx_i == len(line_i['speakers']) - 1:\n",
        "                    line_emo = [label_name.index(emotion_i), label_name.index(first_emotion),\n",
        "                                label_name.index(first_emotion), label_name.index(first_emotion)]\n",
        "                elif idx_i == len(line_i['speakers']) - 2:\n",
        "                    line_emo = [label_name.index(emotion_i), label_name.index(line_i['emotions'][idx_i + 1]),\n",
        "                                label_name.index(first_emotion), label_name.index(first_emotion)]\n",
        "                elif idx_i == len(line_i['speakers']) - 3:\n",
        "                    line_emo = [label_name.index(emotion_i), label_name.index(line_i['emotions'][idx_i + 1]),\n",
        "                                label_name.index(line_i['emotions'][idx_i + 2]), label_name.index(first_emotion)]\n",
        "                else:\n",
        "                    line_emo = [label_name.index(emotion_i), label_name.index(line_i['emotions'][idx_i + 1]),\n",
        "                                label_name.index(line_i['emotions'][idx_i + 2]),\n",
        "                                label_name.index(line_i['emotions'][idx_i + 3])]\n",
        "\n",
        "                train_list_.append(line_emo)\n",
        "\n",
        "    return np.array(train_list_)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    label_name = ['-1', 'disgust', 'contempt', 'anger', 'neutral', 'joy', 'sadness', 'fear', 'surprise']\n",
        "\n",
        "    first_emotion = \"-1\"\n",
        "\n",
        "\n",
        "\n",
        "    X_train, y_train = read_json(train_path)\n",
        "\n",
        "    X_test = read_json_test(test_path)\n",
        "\n",
        "#2 is optimal\n",
        "    model = XGBClassifier(scale_pos_weight=2, random_state=42)\n",
        "\n",
        "\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    from sklearn.metrics import f1_score\n",
        "    import json\n",
        "\n",
        "    # Load the JSON data from the file\n",
        "    with open(test_labels, 'r') as file:\n",
        "        data = json.load(file)\n",
        "\n",
        "    # Initialize an empty list to store the labels\n",
        "    labels_list = []\n",
        "\n",
        "    # Iterate through each entry in the JSON data\n",
        "    for entry in data:\n",
        "        # Extract the labels from the current entry and convert them to float\n",
        "        labels_list.extend([float(label) for label in entry['labels']])\n",
        "\n",
        "\n",
        "    answer_list=[]\n",
        "    for pred_i in y_pred:\n",
        "\n",
        "        answer_list.append(pred_i)\n",
        "\n",
        "    f1 = f1_score(labels_list,answer_list)\n",
        "\n",
        "    # Print the F1 score\n",
        "    print(\"F1 Score:\", f1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "W5bEkOugiZUt",
        "outputId": "8b32b650-90f4-424f-8476-f534889e3f9b"
      },
      "outputs": [
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAqsAAAIjCAYAAAAk+FJEAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAz90lEQVR4nO3deVhUdf//8deAOCDIIqJCKuSSt6a5lqkpmluL5VK53SZaZpaZibZYmUgm/TR3Lcsyuc1Ky9RcutUy09yXUFtuE/e73HBBQVmE8/ujr3M3osno4HyC5+O6uK57zjlzzvvwh/ezwzkzNsuyLAEAAAAG8vL0AAAAAMCVEKsAAAAwFrEKAAAAYxGrAAAAMBaxCgAAAGMRqwAAADAWsQoAAABjEasAAAAwFrEKAAAAYxGrAHAZu3fvVps2bRQUFCSbzaYFCxa4df/79++XzWbTzJkz3brfv7PmzZurefPmnh4DgGGIVQDG2rNnj5588klVqlRJvr6+CgwMVJMmTTRx4kSdP3++QI8dExOjnTt36o033tCsWbPUoEGDAj3ejdSrVy/ZbDYFBgZe9ve4e/du2Ww22Ww2vfXWWy7v//fff1dcXJySkpLcMC2Aoq6YpwcAgMtZsmSJHnnkEdntdvXs2VM1a9ZUVlaWvv/+ez3//PP66aef9N577xXIsc+fP6/169frlVde0TPPPFMgx4iMjNT58+fl4+NTIPu/mmLFiuncuXNatGiROnfu7LRu9uzZ8vX1VUZGxjXt+/fff9eIESMUFRWlOnXq5Pt9y5cvv6bjASjciFUAxtm3b5+6du2qyMhIrVy5UuHh4Y51/fv3V3JyspYsWVJgxz9+/LgkKTg4uMCOYbPZ5OvrW2D7vxq73a4mTZrok08+yROrH3/8se6//37Nmzfvhsxy7tw5lShRQsWLF78hxwPw98JtAACMM3r0aKWlpemDDz5wCtWLqlSpooEDBzpeX7hwQa+//roqV64su92uqKgovfzyy8rMzHR6X1RUlNq1a6fvv/9ed9xxh3x9fVWpUiX961//cmwTFxenyMhISdLzzz8vm82mqKgoSX/8+fzi//6zuLg42Ww2p2UrVqzQXXfdpeDgYAUEBKhatWp6+eWXHeuvdM/qypUr1bRpU/n7+ys4OFjt27fXL7/8ctnjJScnq1evXgoODlZQUJB69+6tc+fOXfkXe4nu3bvrq6++0unTpx3LNm/erN27d6t79+55tj958qSGDBmiWrVqKSAgQIGBgbr33nu1fft2xzarVq3S7bffLknq3bu343aCi+fZvHlz1axZU1u3blWzZs1UokQJx+/l0ntWY2Ji5Ovrm+f827Ztq5CQEP3+++/5PlcAf1/EKgDjLFq0SJUqVVLjxo3ztX2fPn302muvqV69eho/fryio6OVkJCgrl275tk2OTlZDz/8sFq3bq2xY8cqJCREvXr10k8//SRJ6tSpk8aPHy9J6tatm2bNmqUJEya4NP9PP/2kdu3aKTMzU/Hx8Ro7dqwefPBBrV279i/f9/XXX6tt27Y6duyY4uLiFBsbq3Xr1qlJkybav39/nu07d+6ss2fPKiEhQZ07d9bMmTM1YsSIfM/ZqVMn2Ww2ffHFF45lH3/8sf7xj3+oXr16ebbfu3evFixYoHbt2mncuHF6/vnntXPnTkVHRzvCsXr16oqPj5ck9e3bV7NmzdKsWbPUrFkzx35OnDihe++9V3Xq1NGECRPUokWLy843ceJEhYWFKSYmRjk5OZKkd999V8uXL9fkyZMVERGR73MF8DdmAYBBUlNTLUlW+/bt87V9UlKSJcnq06eP0/IhQ4ZYkqyVK1c6lkVGRlqSrNWrVzuWHTt2zLLb7dbgwYMdy/bt22dJssaMGeO0z5iYGCsyMjLPDMOHD7f+/M/p+PHjLUnW8ePHrzj3xWN8+OGHjmV16tSxypQpY504ccKxbPv27ZaXl5fVs2fPPMd77LHHnPbZsWNHKzQ09IrH/PN5+Pv7W5ZlWQ8//LDVsmVLy7IsKycnxypXrpw1YsSIy/4OMjIyrJycnDznYbfbrfj4eMeyzZs35zm3i6Kjoy1J1rRp0y67Ljo62mnZsmXLLEnWyJEjrb1791oBAQFWhw4drnqOAAoPrqwCMMqZM2ckSSVLlszX9kuXLpUkxcbGOi0fPHiwJOW5t7VGjRpq2rSp43VYWJiqVaumvXv3XvPMl7p4r+vChQuVm5ubr/ccPnxYSUlJ6tWrl0qVKuVYftttt6l169aO8/yzfv36Ob1u2rSpTpw44fgd5kf37t21atUqHTlyRCtXrtSRI0cuewuA9Md9rl5ef/zfRk5Ojk6cOOG4xWHbtm35Pqbdblfv3r3ztW2bNm305JNPKj4+Xp06dZKvr6/efffdfB8LwN8fsQrAKIGBgZKks2fP5mv7AwcOyMvLS1WqVHFaXq5cOQUHB+vAgQNOyytWrJhnHyEhITp16tQ1TpxXly5d1KRJE/Xp00dly5ZV165dNXfu3L8M14tzVqtWLc+66tWrKyUlRenp6U7LLz2XkJAQSXLpXO677z6VLFlSc+bM0ezZs3X77bfn+V1elJubq/Hjx6tq1aqy2+0qXbq0wsLCtGPHDqWmpub7mDfddJNLD1O99dZbKlWqlJKSkjRp0iSVKVMm3+8F8PdHrAIwSmBgoCIiIvTjjz+69L5LH3C6Em9v78sutyzrmo9x8X7Ki/z8/LR69Wp9/fXXevTRR7Vjxw516dJFrVu3zrPt9biec7nIbrerU6dOSkxM1Pz58694VVWSRo0apdjYWDVr1kwfffSRli1bphUrVujWW2/N9xVk6Y/fjyt++OEHHTt2TJK0c+dOl94L4O+PWAVgnHbt2mnPnj1av379VbeNjIxUbm6udu/e7bT86NGjOn36tOPJfncICQlxenL+okuv3kqSl5eXWrZsqXHjxunnn3/WG2+8oZUrV+rbb7+97L4vzrlr16486/7zn/+odOnS8vf3v74TuILu3bvrhx9+0NmzZy/7UNpFn3/+uVq0aKEPPvhAXbt2VZs2bdSqVas8v5P8/odDfqSnp6t3796qUaOG+vbtq9GjR2vz5s1u2z8A8xGrAIzzwgsvyN/fX3369NHRo0fzrN+zZ48mTpwo6Y8/Y0vK88T+uHHjJEn333+/2+aqXLmyUlNTtWPHDseyw4cPa/78+U7bnTx5Ms97L344/qUfp3VReHi46tSpo8TERKf4+/HHH7V8+XLHeRaEFi1a6PXXX9eUKVNUrly5K27n7e2d56rtZ599pt9++81p2cWovlzYu+rFF1/UwYMHlZiYqHHjxikqKkoxMTFX/D0CKHz4UgAAxqlcubI+/vhjdenSRdWrV3f6Bqt169bps88+U69evSRJtWvXVkxMjN577z2dPn1a0dHR2rRpkxITE9WhQ4crfizStejatatefPFFdezYUc8++6zOnTund955R7fccovTA0bx8fFavXq17r//fkVGRurYsWN6++23Vb58ed11111X3P+YMWN07733qlGjRnr88cd1/vx5TZ48WUFBQYqLi3PbeVzKy8tLr7766lW3a9euneLj49W7d281btxYO3fu1OzZs1WpUiWn7SpXrqzg4GBNmzZNJUuWlL+/vxo2bKibb77ZpblWrlypt99+W8OHD3d8lNaHH36o5s2ba9iwYRo9erRL+wPw98SVVQBGevDBB7Vjxw49/PDDWrhwofr376+XXnpJ+/fv19ixYzVp0iTHtu+//75GjBihzZs367nnntPKlSs1dOhQffrpp26dKTQ0VPPnz1eJEiX0wgsvKDExUQkJCXrggQfyzF6xYkXNmDFD/fv319SpU9WsWTOtXLlSQUFBV9x/q1at9O9//1uhoaF67bXX9NZbb+nOO+/U2rVrXQ69gvDyyy9r8ODBWrZsmQYOHKht27ZpyZIlqlChgtN2Pj4+SkxMlLe3t/r166du3brpu+++c+lYZ8+e1WOPPaa6devqlVdecSxv2rSpBg4cqLFjx2rDhg1uOS8AZrNZrtyJDwAAANxAXFkFAACAsYhVAAAAGItYBQAAgLGIVQAAABiLWAUAAICxiFUAAAAYi1gFAACAsQrlN1j51X3G0yMAgFud2jzF0yMAgFv55rNCubIKAAAAYxGrAAAAMBaxCgAAAGMRqwAAADAWsQoAAABjEasAAAAwFrEKAAAAYxGrAAAAMBaxCgAAAGMRqwAAADAWsQoAAABjEasAAAAwFrEKAAAAYxGrAAAAMBaxCgAAAGMRqwAAADAWsQoAAABjEasAAAAwFrEKAAAAYxGrAAAAMBaxCgAAAGMRqwAAADAWsQoAAABjEasAAAAwFrEKAAAAYxGrAAAAMBaxCgAAAGMRqwAAADAWsQoAAABjEasAAAAwFrEKAAAAYxGrAAAAMBaxCgAAAGMRqwAAADAWsQoAAABjEasAAAAwFrEKAAAAYxGrAAAAMBaxCgAAAGMRqwAAADAWsQoAAABjEasAAAAwFrEKAAAAYxGrAAAAMBaxCgAAAGMRqwAAADAWsQoAAABjEasAAAAwFrEKAAAAYxGrAAAAMBaxCgAAAGMRqwAAADAWsQoAAABjEasAAAAwFrEKAAAAYxGrAAAAMBaxCgAAAGMRqwAAADAWsQoAAABjEasAAAAwFrEKAAAAYxGrAAAAMBaxCgAAAGMRqwAAADAWsQoAAABjEasAAAAwFrEKAAAAYxGrAAAAMBaxCgAAAGMRqwAAADAWsQoAAABjEasAAAAwFrEKAAAAYxGrAAAAMBaxCgAAAGMRqwAAADAWsQoAAABjEasAAAAwFrEKAAAAYxGrAAAAMBaxCgAAAGMRqwAAADAWsQoAAABjEasAAAAwFrEKAAAAYxGrAAAAMBaxCgAAAGMRqwAAADAWsQoAAABjEasAAAAwFrEKAAAAYxGrAAAAMBaxCgAAAGMRqwAAADAWsQoAAABjEasAAAAwFrEKAAAAYxGrAAAAMBaxCgAAAGMRqwAAADAWsQoAAABjEasAAAAwFrEKAAAAYxGrAAAAMBaxCgAAAGMRqwAAADAWsQoAAABjEasAAAAwFrEKAAAAYxXz9ACAJ/1nyQhFRoTmWT5tzmrFv71Yw566Xy3v/IcqlAtRyqk0LVq1QyPeXqwzaRmObSuUC9HEl7sousEtSjufqdmLNmrY5C+Vk5Pr2Ka4TzG93Pdedbv/dpUNLakjKWc06r2v9K+FG27IeQLAnx09elQTxo3R2jVrlJFxXhUqRip+5CjdWrOWJOlceromjB+rb1d+rdTTp3XTTeXVrcej6tylm4cnR1FErKJIu6vHGHl72Ryva1SJ0NJpA/TFih8UHhak8LAgDR0/X7/sPaKK4aU0+ZWuCg8LUvfnP5AkeXnZ9MWkp3T0xBm16DVW5cKC9P7rjyr7Qo6GT1nk2O9Hox9T2VIl1W/EbO05eFzhYUHystnyzAMABe1Maqp69eimBnc01NRp0xVSKkQHDxxQYGCQY5u3Rr+pTRs3aNSbYxRx001av3atRo0coTJhZdT87pYenB5FEbGKIi3lVJrT6yG9a2rPweNas3W3JKnbkPcd6/b9N0VxUxZpxhs95e3tpZycXLVqVF3VK5XT/f0m69jJs9rx62+Kf3uJRj7bXiOnLVX2hRy1blxdTetXUY12cTp15pwk6eDhkzfuJAHgT2Z8MF1ly5XT628kOJaVL1/BaZukpB/0QPsOuv2OhpKkhzt30eefzdGPO3cQq7jhPHrPakpKikaPHq2OHTuqUaNGatSokTp27KgxY8bo+PHjnhwNRZBPMW91ve92JS5cf8VtAkv66kx6huNP/A1vu1k/Jv+uYyfPOrZZse4XBZX0U43K4ZKk+6NradvPBxXbq5X2LBupHQteU8KgjvK1+xTsCQHAZXz37UrdemtNDRn0rJo3baTOD3XQvM/mOm1Tp05dffftSh09elSWZWnTxg06sH+fGjW5y0NToyjz2JXVzZs3q23btipRooRatWqlW265RdIf99FMmjRJb775ppYtW6YGDRr85X4yMzOVmZnptMzKzZHNy7vAZkfh9GCL2xRc0k8fLdp42fWhwf4a+sS9mjFvnWNZ2dBAHTtx1mm7YyfP/LGudKC0S7r5ptJqXKeyMjIvqEvsdIWG+Gvi0C4qFeSvJ+M+KrgTAoDL+O9/D2nunE/0aExvPd63n37auVP/L2GkfHx89GCHjpKkl14Zpvjhw9Tm7mYqVqyYbDabho8YqfoNbvfw9CiKPBarAwYM0COPPKJp06bJdsm9e5ZlqV+/fhowYIDWr7/yVS5JSkhI0IgRI5yWeZe9XT7hd7h9ZhRuMR0aa9nan3X4eGqedSX9fTV/0lP6Ze9hjXx3iUv79fKyybIs9X5lpuPBrBfHfqGPxzyugQlzlJGZ7Zb5ASA/cnMt3Vqzpp59LlaSVL16DSUn79Zncz91xOons2dpx44kTZzyjiIiIrR1yxaNGjlCYWXK6M5GjT05Poogj90GsH37dg0aNChPqEqSzWbToEGDlJSUdNX9DB06VKmpqU4/xcrWL4CJUZhVDA/R3Q2raeaCdXnWBZSw68upT+vsuQx1iZ2uCxf+95T/0RNnVCa0pNP2ZUoF/rEu5Y8rrEdSzuj3Y6lOnyDwn31H5OXlpZvKBhfA2QDAlYWFhalS5cpOyypVqqTDh3+XJGVkZGjShPEa8sJQNW9xt26p9g91+2cPtb33PiV++IEnRkYR57FYLVeunDZt2nTF9Zs2bVLZsmWvuh+73a7AwECnH24BgKsefbCRjp08q6/W/OS0vKS/rxa/84yysnP08HPvKjPrgtP6jTv2qWaVCIWFBDiWtbzzH0o9e16/7D0iSVqftFfhYUHy9yvu2KZqZBnl5OTqt6OnC+6kAOAy6tStp/379jktO7B/vyIibpIkXbhwQRcuZMvLy/likpeXt3It64bNCVzksdsAhgwZor59+2rr1q1q2bKlI0yPHj2qb775RtOnT9dbb73lqfFQhNhsNvVsf6dmL97o9NmoJf19tfjt/vLzLa7eryQq0N9Xgf6+kqTjp9KUm2vp6/W/6Je9R/TByBi9MnGByoYGanj/dnp37mplZf8RtnO+2qyhT9yj90b00OvTlio02F+jnuuoxIXruQUAwA3Xo2eMYnp00/vvTVObtvfqx5079Pnnc/VaXLwkKSAgQA1uv0Pj3hoju91X4RER2rp5sxZ/uUBDXnjJw9OjKLJZluf+M2nOnDkaP368tm7dqpycHEmSt7e36tevr9jYWHXu3Pma9utX9xl3jolCruWd/9Did55RrfbxSj54zLG8af2qWv7+wMu+p9p9rzk+fqpieIgmvtxVzepXVXpGpmYv2qRXJy10Ct9bospq3IuPqFHtSjqZmq55K7YpbupiYhX5dmrzFE+PgELku1XfatKEcTp4YL9uKl9ej/bsrYce+d//56YcP66JE8Zp/brvdSY1VeEREXro4S56NKbXZW/fA66Fbz4vmXo0Vi/Kzs5WSkqKJKl06dLy8bm+j/QhVgEUNsQqgMImv7FqxJcC+Pj4KDw83NNjAAAAwDAe/VIAAAAA4K8QqwAAADAWsQoAAABjEasAAAAwFrEKAAAAYxGrAAAAMBaxCgAAAGMRqwAAADAWsQoAAABjEasAAAAwFrEKAAAAYxGrAAAAMBaxCgAAAGMRqwAAADAWsQoAAABjEasAAAAwFrEKAAAAYxGrAAAAMBaxCgAAAGMRqwAAADAWsQoAAABjEasAAAAwFrEKAAAAYxGrAAAAMBaxCgAAAGMRqwAAADAWsQoAAABjEasAAAAwFrEKAAAAYxGrAAAAMBaxCgAAAGMRqwAAADAWsQoAAABjEasAAAAwFrEKAAAAYxGrAAAAMBaxCgAAAGMRqwAAADAWsQoAAABjEasAAAAwFrEKAAAAYxGrAAAAMBaxCgAAAGMRqwAAADAWsQoAAABjEasAAAAwFrEKAAAAYxGrAAAAMBaxCgAAAGMRqwAAADAWsQoAAABjEasAAAAwFrEKAAAAYxGrAAAAMBaxCgAAAGMRqwAAADAWsQoAAABjEasAAAAwFrEKAAAAYxGrAAAAMBaxCgAAAGMRqwAAADAWsQoAAABjEasAAAAwFrEKAAAAYxGrAAAAMBaxCgAAAGMRqwAAADAWsQoAAABjEasAAAAwFrEKAAAAYxGrAAAAMJbLsZqYmKglS5Y4Xr/wwgsKDg5W48aNdeDAAbcOBwAAgKLN5VgdNWqU/Pz8JEnr16/X1KlTNXr0aJUuXVqDBg1y+4AAAAAouoq5+oZDhw6pSpUqkqQFCxbooYceUt++fdWkSRM1b97c3fMBAACgCHP5ympAQIBOnDghSVq+fLlat24tSfL19dX58+fdOx0AAACKNJevrLZu3Vp9+vRR3bp19euvv+q+++6TJP3000+Kiopy93wAAAAowly+sjp16lQ1atRIx48f17x58xQaGipJ2rp1q7p16+b2AQEAAFB02SzLsjw9hLv51X3G0yMAgFud2jzF0yMAgFv55vPv+/nabMeOHfk+8G233ZbvbQEAAIC/kq9YrVOnjmw2m650EfbiOpvNppycHLcOCAAAgKIrX7G6b9++gp4DAAAAyCNfsRoZGVnQcwAAAAB5uPxpAJI0a9YsNWnSRBEREY6vWJ0wYYIWLlzo1uEAAABQtLkcq++8845iY2N133336fTp0457VIODgzVhwgR3zwcAAIAizOVYnTx5sqZPn65XXnlF3t7ejuUNGjTQzp073TocAAAAijaXY3Xfvn2qW7dunuV2u13p6eluGQoAAACQriFWb775ZiUlJeVZ/u9//1vVq1d3x0wAAACApHx+GsCfxcbGqn///srIyJBlWdq0aZM++eQTJSQk6P333y+IGQEAAFBEuRyrffr0kZ+fn1599VWdO3dO3bt3V0REhCZOnKiuXbsWxIwAAAAoomzWlb6WKh/OnTuntLQ0lSlTxp0zXTe/us94egQAcKtTm6d4egQAcCvffF4ydfnK6kXHjh3Trl27JP3xdathYWHXuisAAADgslx+wOrs2bN69NFHFRERoejoaEVHRysiIkI9evRQampqQcwIAACAIsrlWO3Tp482btyoJUuW6PTp0zp9+rQWL16sLVu26MknnyyIGQEAAFBEuXzPqr+/v5YtW6a77rrLafmaNWt0zz33GPFZq9yzCqCw4Z5VAIVNfu9ZdfnKamhoqIKCgvIsDwoKUkhIiKu7AwAAAK7I5Vh99dVXFRsbqyNHjjiWHTlyRM8//7yGDRvm1uEAAABQtOXrAmzdunVls9kcr3fv3q2KFSuqYsWKkqSDBw/Kbrfr+PHj3LcKAAAAt8lXrHbo0KGAxwAAAADyuq4vBTAVD1gBKGx4wApAYVNgD1gBAAAAN4rL32CVk5Oj8ePHa+7cuTp48KCysrKc1p88edJtwwEAAKBoc/nK6ogRIzRu3Dh16dJFqampio2NVadOneTl5aW4uLgCGBEAAABFlcuxOnv2bE2fPl2DBw9WsWLF1K1bN73//vt67bXXtGHDhoKYEQAAAEWUy7F65MgR1apVS5IUEBCg1NRUSVK7du20ZMkS904HAACAIs3lWC1fvrwOHz4sSapcubKWL18uSdq8ebPsdrt7pwMAAECR5nKsduzYUd98840kacCAARo2bJiqVq2qnj176rHHHnP7gAAAACi6rvtzVjds2KB169apatWqeuCBB9w113Xhc1YBFDZ8ziqAwuaGfc7qnXfeqdjYWDVs2FCjRo263t0BAAAADm77Bqvt27erXr16ysnJccfursv5bE9PAADulZ2T6+kRAMCtAn3zd82Ub7ACAACAsYhVAAAAGItYBQAAgLHy+RyWFBsb+5frjx8/ft3DAAAAAH+W71j94YcfrrpNs2bNrmsYAAAA4M/c9mkAJuHTAAAUNnwaAIDChk8DAAAAwN8esQoAAABjEasAAAAwFrEKAAAAYxGrAAAAMNY1xeqaNWvUo0cPNWrUSL/99pskadasWfr+++/dOhwAAACKNpdjdd68eWrbtq38/Pz0ww8/KDMzU5KUmpqqUaNGuX1AAAAAFF0ux+rIkSM1bdo0TZ8+XT4+Po7lTZo00bZt29w6HAAAAIo2l2N1165dl/2mqqCgIJ0+fdodMwEAAACSriFWy5Urp+Tk5DzLv//+e1WqVMktQwEAAADSNcTqE088oYEDB2rjxo2y2Wz6/fffNXv2bA0ZMkRPPfVUQcwIAACAIqqYq2946aWXlJubq5YtW+rcuXNq1qyZ7Ha7hgwZogEDBhTEjAAAACiibJZlWdfyxqysLCUnJystLU01atRQQECAu2e7ZuezPT0BALhXdk6up0cAALcK9M3fH/ivOVZNRqwCKGyIVQCFTX5j1eXbAFq0aCGbzXbF9StXrnR1lwAAAMBluRyrderUcXqdnZ2tpKQk/fjjj4qJiXHXXAAAAIDrsTp+/PjLLo+Li1NaWtp1DwQAAABc5LZ7VpOTk3XHHXfo5MmT7tjddeGeVQCFDfesAihs8nvPqsufs3ol69evl6+vr7t2BwAAALh+G0CnTp2cXluWpcOHD2vLli0aNmyY2wYDAAAAXI7VoKAgp9deXl6qVq2a4uPj1aZNG7cNBgAAALh0z2pOTo7Wrl2rWrVqKSQkpCDnui7cswqgsOGeVQCFTYHcs+rt7a02bdro9OnT1zITAAAA4BKXH7CqWbOm9u7dWxCzAAAAAE5cjtWRI0dqyJAhWrx4sQ4fPqwzZ844/QAAAADuku97VuPj4zV48GCVLFnyf2/+09euWpYlm82mnJwc90/pIu5ZBVDYcM8qgMImv/es5jtWvb29dfjwYf3yyy9/uV10dHS+DlyQiFUAhQ2xCqCwyW+s5vujqy42rQkxCgAAgKLBpXtW//xnfwAAAKCg5fs2AC8vLwUFBV01WE+ePOmWwa4HtwEAKGy4DQBAYeP22wAkacSIEXm+wQoAAAAoKC5dWT1y5IjKlClT0DNdN66sAihsuLIKoLBx+zdYcb8qAAAAbrR8x2o+L8ACAAAAbpPve1Zzc/kTFAAAAG4sl79uFQAAALhRiFUAAAAYi1gFAACAsYhVAAAAGItYBQAAgLGIVQAAABiLWAUAAICxiFUAAAAYi1gFAACAsYhVAAAAGItYBQAAgLGIVQAAABiLWAUAAICxiFUAAAAYi1gFAACAsYhVAAAAGItYBQAAgLGIVQAAABiLWAUAAICxiFUAAAAYi1gFAACAsYhVAAAAGItYBQAAgLGIVQAAABiLWAUAAICxiFUAAAAYi1gFAACAsYhVAAAAGItYBQAAgLGIVQAAABiLWAUAAICxiFUAAAAYi1gFAACAsYhVAAAAGItYBQAAgLGIVQAAABiLWAUAAICxiFUAAAAYi1gFAACAsYhVAAAAGItYBQAAgLGIVQAAABiLWAUAAICxiFUAAAAYi1gFAACAsYhVAAAAGItYBQAAgLGIVQAAABiLWAUAAICxiFUAAAAYi1gFAACAsYhVAAAAGItYBQAAgLGIVQAAABiLWAUAAICxiFUAAAAYi1gFAACAsYhVAAAAGItYBQAAgLGIVQAAABiLWAUAAICxiFUAAAAYi1gFAACAsYhVAAAAGItYBQAAgLGIVQAAABiLWAUAAICxiFUAAAAYq5inBwBMl56epqmTJ+rbb77WyZMnVO0fNfTCSy+rZq3bJEknUlI0Yfxb2rDue509e1b16jfQiy8PU2RklGcHBwBJn8/9RPPmfqrDv/8mSapUuYoef/JpNbmrmSTpi8/natlXi7Xrl5+Vnp6ulWs2qmRgoOP9WzdvUr8+MZfd98zZc3VrzVoFfxIo0myWZVmeHsLdzmd7egIUJi8Mfk7Jybv1yrA4hZUpoyWLvtTsWTM1b+FSlSlTRjE9uqpYsWKKHfKiAgICNOtfM7Xu+zX6YuES+ZUo4enxUUhk5+R6egT8Ta1e9a28vb1UoWKkLMvSkkULNWvmDH00Z54qV6mqjz9KVFZmliRp6qRxeWI1OztLqampTvucNnWSNm/coAVLlstms93Q80HhEeibvz/wE6vAX8jIyFCThvU0ftLbahbd3LG8W+dOanJXUz3wYAe1b3ePPl+wWFWqVJUk5ebmqmXzJhrwbKw6PfyIhyZHYUOswp1aNr1Tzw4aovadHnYsu3gF9dJYvdSF7Gzd17q5Onf7p/o8+fSNGBeFVH5jlXtWgb+Qk3NBOTk5stvtTsvtdrt+2LZNWVl/XI2wF//fei8vLxX3Ka4ffth6Q2cFgKvJycnR8q+W6Pz5c6pVu8417WP1d98qNfW0HujQyb3DAVdgdKweOnRIjz322F9uk5mZqTNnzjj9ZGZm3qAJUdj5+wfottp19d60t3Xs2FHl5ORoyaKF2rE9SSkpxxR1cyWFh0do0sSxOpOaquzsLH34wXs6evSIUo4f9/T4ACBJSt79q5rdWV9Nbq+thDdGaMz4yapUuco17Wvh/M91Z+MmKlu2nJunBC7P6Fg9efKkEhMT/3KbhIQEBQUFOf2M+X8JN2hCFAVvJIyWZKnN3c10R71a+nj2LN1z7/3ysnnJx8dHYydM1oH9+9WsyR26s0Edbd60UU2aNpOXF/dxATBDZFSUZs/9Qh9+NEcPPdJVccOGau+eZJf3c/ToEW1Yt1btOz589Y0BN/HopwF8+eWXf7l+7969V93H0KFDFRsb67Qs18t+ha0B11WoWFEfzPxI58+dU1p6msLCyuiFwc/ppvIVJEk1bq2pufMW6uzZs8rOzlapUqXUo9sjqnFrTQ9PDgB/8PEprgoVIyVJ1Wvcqp9/2qlPZ8/Sy6+NcGk/ixZ8oaCgYDWLblEQYwKX5dFY7dChg2w2m/7qGa+rPWVot9vz3E/IA1YoCH4lSsivRAmdSU3VunXf67nY553WlyxZUpJ04MB+/fzTj3r6mYGeGBMArsrKtZSVneXaeyxLixbO130PtFcxH58CmgzIy6OxGh4errffflvt27e/7PqkpCTVr1//Bk8FOFu3do0sy1JU1M06ePCgxo8drZtvrqT2//dwwfJlXykkpJTCwyO0e/cujX5zlFrc3UqNm9zl4ckBQJoycZwa39VU5cpF6Ny5dP176WJt3bJJk9+ZLklKSTmuEykpOnTogCQpOflXlSjhr3Lh4QoKCnbsZ/OmDfr9t/+qQyduAcCN5dFYrV+/vrZu3XrFWL3aVVfgRjh79qwmTxino0ePKCgoWC1bt9Ezzw6Sz/9dWUg5flxjR7+pEydOKCwsTO0ebK++/fg4FwBmOHXyhOJefUkpx48rIKCkqtxyiya/M10NGzWRJH3x2RxNnzbVsX3f3o9Kkl6LH6UH2nd0LP9y/jzdVqeuom6udGNPAEWeRz9ndc2aNUpPT9c999xz2fXp6enasmWLoqOjXdovtwEAKGz4nFUAhQ1fCgAAhQixCqCw4UsBAAAA8LdHrAIAAMBYxCoAAACMRawCAADAWMQqAAAAjEWsAgAAwFjEKgAAAIxFrAIAAMBYxCoAAACMRawCAADAWMQqAAAAjEWsAgAAwFjEKgAAAIxFrAIAAMBYxCoAAACMRawCAADAWMQqAAAAjEWsAgAAwFjEKgAAAIxFrAIAAMBYxCoAAACMRawCAADAWMQqAAAAjEWsAgAAwFjEKgAAAIxFrAIAAMBYxCoAAACMRawCAADAWMQqAAAAjEWsAgAAwFjEKgAAAIxFrAIAAMBYxCoAAACMRawCAADAWMQqAAAAjEWsAgAAwFjEKgAAAIxFrAIAAMBYxCoAAACMRawCAADAWMQqAAAAjEWsAgAAwFjEKgAAAIxFrAIAAMBYxCoAAACMRawCAADAWMQqAAAAjEWsAgAAwFjEKgAAAIxFrAIAAMBYxCoAAACMRawCAADAWMQqAAAAjEWsAgAAwFjEKgAAAIxFrAIAAMBYxCoAAACMRawCAADAWMQqAAAAjEWsAgAAwFjEKgAAAIxFrAIAAMBYxCoAAACMRawCAADAWMQqAAAAjEWsAgAAwFjEKgAAAIxFrAIAAMBYxCoAAACMRawCAADAWMQqAAAAjEWsAgAAwFjEKgAAAIxFrAIAAMBYxCoAAACMRawCAADAWMQqAAAAjEWsAgAAwFjEKgAAAIxFrAIAAMBYxCoAAACMRawCAADAWMQqAAAAjEWsAgAAwFjEKgAAAIxFrAIAAMBYxCoAAACMRawCAADAWMQqAAAAjEWsAgAAwFjEKgAAAIxFrAIAAMBYxCoAAACMRawCAADAWMQqAAAAjEWsAgAAwFjEKgAAAIxFrAIAAMBYxCoAAACMRawCAADAWMQqAAAAjEWsAgAAwFjEKgAAAIxFrAIAAMBYxCoAAACMRawCAADAWMQqAAAAjEWsAgAAwFjEKgAAAIxlsyzL8vQQwN9RZmamEhISNHToUNntdk+PAwDXjX/XYCJiFbhGZ86cUVBQkFJTUxUYGOjpcQDguvHvGkzEbQAAAAAwFrEKAAAAYxGrAAAAMBaxClwju92u4cOH8xACgEKDf9dgIh6wAgAAgLG4sgoAAABjEasAAAAwFrEKAAAAYxGrAAAAMBaxClyjqVOnKioqSr6+vmrYsKE2bdrk6ZEA4JqsXr1aDzzwgCIiImSz2bRgwQJPjwQ4EKvANZgzZ45iY2M1fPhwbdu2TbVr11bbtm117NgxT48GAC5LT09X7dq1NXXqVE+PAuTBR1cB16Bhw4a6/fbbNWXKFElSbm6uKlSooAEDBuill17y8HQAcO1sNpvmz5+vDh06eHoUQBJXVgGXZWVlaevWrWrVqpVjmZeXl1q1aqX169d7cDIAAAofYhVwUUpKinJyclS2bFmn5WXLltWRI0c8NBUAAIUTsQoAAABjEauAi0qXLi1vb28dPXrUafnRo0dVrlw5D00FAEDhRKwCLipevLjq16+vb775xrEsNzdX33zzjRo1auTByQAAKHyKeXoA4O8oNjZWMTExatCgge644w5NmDBB6enp6t27t6dHAwCXpaWlKTk52fF63759SkpKUqlSpVSxYkUPTgbw0VXANZsyZYrGjBmjI0eOqE6dOpo0aZIaNmzo6bEAwGWrVq1SixYt8iyPiYnRzJkzb/xAwJ8QqwAAADAW96wCAADAWMQqAAAAjEWsAgAAwFjEKgAAAIxFrAIAAMBYxCoAAACMRawCAADAWMQqAAAAjEWsAoCLevXqpQ4dOjheN2/eXM8999wNn2PVqlWy2Ww6ffp0gR3j0nO9FjdiTgCFF7EKoFDo1auXbDabbDabihcvripVqig+Pl4XLlwo8GN/8cUXev311/O17Y0Ot6ioKE2YMOGGHAsACkIxTw8AAO5yzz336MMPP1RmZqaWLl2q/v37y8fHR0OHDs2zbVZWlooXL+6W45YqVcot+wEA5MWVVQCFht1uV7ly5RQZGamnnnpKrVq10pdffinpf3/OfuONNxQREaFq1apJkg4dOqTOnTsrODhYpUqVUvv27bV//37HPnNychQbG6vg4GCFhobqhRdekGVZTse99DaAzMxMvfjii6pQoYLsdruqVKmiDz74QPv371eLFi0kSSEhIbLZbOrVq5ckKTc3VwkJCbr55pvl5+en2rVr6/PPP3c6ztKlS3XLLbfIz89PLVq0cJrzWuTk5Ojxxx93HLNatWqaOHHiZbcdMWKEwsLCFBgYqH79+ikrK8uxLj+z/9mBAwf0wAMPKCQkRP7+/rr11lu1dOnS6zoXAIUXV1YBFFp+fn46ceKE4/U333yjwMBArVixQpKUnZ2ttm3bqlGjRlqzZo2KFSumkSNH6p577tGOHTtUvHhxjR07VjNnztSMGTNUvXp1jR07VvPnz9fdd999xeP27NlT69ev16RJk1S7dm3t27dPKSkpqlChgubNm6eHHnpIu3btUmBgoPz8/CRJCQkJ+uijjzRt2jRVrVpVq1evVo8ePRQWFqbo6GgdOnRInTp1Uv/+/dW3b19t2bJFgwcPvq7fT25ursqXL6/PPvtMoaGhWrdunfr27avw8HB17tzZ6ffm6+urVatWaf/+/erdu7dCQ0P1xhtv5Gv2S/Xv319ZWVlavXq1/P399fPPPysgIOC6zgVAIWYBQCEQExNjtW/f3rIsy8rNzbVWrFhh2e12a8iQIY71ZcuWtTIzMx3vmTVrllWtWjUrNzfXsSwzM9Py8/Ozli1bZlmWZYWHh1ujR492rM/OzrbKly/vOJZlWVZ0dLQ1cOBAy7Isa9euXZYka8WKFZed89tvv7UkWadOnXIsy8jIsEqUKGGtW7fOadvHH3/c6tatm2VZljV06FCrRo0aTutffPHFPPu6VGRkpDV+/Pgrrr9U//79rYceesjxOiYmxipVqpSVnp7uWPbOO+9YAQEBVk5OTr5mv/Sca9WqZcXFxeV7JgBFG1dWARQaixcvVkBAgLKzs5Wbm6vu3bsrLi7Osb5WrVpO96lu375dycnJKlmypNN+MjIytGfPHqWmpurw4cNq2LChY12xYsXUoEGDPLcCXJSUlCRvb+/LXlG8kuTkZJ07d06tW7d2Wp6VlaW6detKkn755RenOSSpUaNG+T7GlUydOlUzZszQwYMHdf78eWVlZalOnTpO29SuXVslSpRwOm5aWpoOHTqktLS0q85+qWeffVZPPfWUli9frlatWumhhx7Sbbfddt3nAqBwIlYBFBotWrTQO++8o+LFiysiIkLFijn/E+fv7+/0Oi0tTfXr19fs2bPz7CssLOyaZrj4Z31XpKWlSZKWLFmim266yWmd3W6/pjny49NPP9WQIUM0duxYNWrUSCVLltSYMWO0cePGfO/jWmbv06eP2rZtqyVLlmj58uVKSEjQ2LFjNWDAgGs/GQCFFrEKoNDw9/dXlSpV8r19vXr1NGfOHJUpU0aBgYGX3SY8PFwbN25Us2bNJEkXLlzQ1q1bVa9evctuX6tWLeXm5uq7775Tq1at8qy/eGU3JyfHsaxGjRqy2+06ePDgFa/IVq9e3fGw2EUbNmy4+kn+hbVr16px48Z6+umnHcv27NmTZ7vt27fr/PnzjhDfsGGDAgICVKFCBZUqVeqqs19OhQoV1K9fP/Xr109Dhw7V9OnTiVUAl8WnAQAosv75z3+qdOnSat++vdasWaN9+/Zp1apVevbZZ/Xf//5XkjRw4EC9+eabWrBggf7zn//o6aef/svPSI2KilJMTIwee+wxLViwwLHPuXPnSpIiIyNls9m0ePFiHT9+XGlpaSpZsqSGDBmiQYMGKTExUXv27NG2bds0efJkJSYmSpL69eun3bt36/nnn9euXbv08ccfa+bMmfk6z99++01JSUlOP6dOnVLVqlW1ZcsWLVu2TL/++quGDRumzZs353l/VlaWHn/8cf38889aunSphg8frmeeeUZeXl75mv1Szz33nJYtW6Z9+/Zp27Zt+vbbb1W9evV8nQuAIsjTN80CgDv8+QErV9YfPnzY6tmzp1W6dGnLbrdblSpVsp544gkrNTXVsqw/HqgaOHCgFRgYaAUHB1uxsbFWz549r/iAlWVZ1vnz561BgwZZ4eHhVvHixa0qVapYM2bMcKyPj4+3ypUrZ9lsNismJsayrD8eCpswYYJVrVo1y8fHxwoLC7Patm1rfffdd473LVq0yKpSpYplt9utpk2bWjNmzMjXA1aS8vzMmjXLysjIsHr16mUFBQVZwcHB1lNPPWW99NJLVu3atfP83l577TUrNDTUCggIsJ544gkrIyPDsc3VZr/0AatnnnnGqly5smW3262wsDDr0UcftVJSUq54DgCKNptlXeEpAQAAAMDDuA0AAAAAxiJWAQAAYCxiFQAAAMYiVgEAAGAsYhUAAADGIlYBAABgLGIVAAAAxiJWAQAAYCxiFQAAAMYiVgEAAGAsYhUAAADG+v9o/T3X++7MewAAAABJRU5ErkJggg==",
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Compute the confusion matrix\n",
        "conf_matrix = confusion_matrix(labels_list, answer_list)\n",
        "\n",
        "# Create a heatmap for visualization\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False)\n",
        "\n",
        "# Add labels and title\n",
        "plt.xlabel(\"Predicted Labels\")\n",
        "plt.ylabel(\"True Labels\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zGSYaNA_HFMC"
      },
      "source": [
        "RANDOM FOREST CLASSIFIER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uyez1qlZRxTQ",
        "outputId": "8a311872-a16d-4802-f5eb-6a2e569d3733"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "F1 Score: 0.783817951959545\n"
          ]
        }
      ],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import joblib\n",
        "import json\n",
        "import numpy as np\n",
        "\n",
        "# Save the trained model\n",
        "\n",
        "\n",
        "def read_json(path):\n",
        "    train_list_ = []\n",
        "    label_list_ = []\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as file_read:\n",
        "        data_line = json.loads(file_read.read())\n",
        "\n",
        "\n",
        "        for idx, line_i in enumerate(data_line):\n",
        "\n",
        "\n",
        "            for idx_i, speaker_i, utterances_i, emotion_i, trigger_i in zip(range(len(line_i['speakers'])),\n",
        "                                                                            line_i['speakers'],\n",
        "                                                                            line_i['utterances'],\n",
        "                                                                            line_i['emotions'], line_i['triggers']):\n",
        "                if idx_i == len(line_i['speakers']) - 1:\n",
        "                    line_emo = [label_name.index(emotion_i), label_name.index(first_emotion),\n",
        "                                label_name.index(first_emotion), label_name.index(first_emotion), label_name.index(first_emotion)]\n",
        "                elif idx_i == len(line_i['speakers']) - 2:\n",
        "                    line_emo = [label_name.index(emotion_i), label_name.index(line_i['emotions'][idx_i + 1]),\n",
        "                                label_name.index(first_emotion), label_name.index(first_emotion), label_name.index(first_emotion)]\n",
        "                elif idx_i == len(line_i['speakers']) - 3:\n",
        "                    line_emo = [label_name.index(emotion_i), label_name.index(line_i['emotions'][idx_i + 1]),\n",
        "                                label_name.index(line_i['emotions'][idx_i + 2]), label_name.index(first_emotion), label_name.index(first_emotion)]\n",
        "                elif idx_i == len(line_i['speakers']) - 4:\n",
        "                    line_emo = [label_name.index(emotion_i), label_name.index(line_i['emotions'][idx_i + 1]),\n",
        "                                label_name.index(line_i['emotions'][idx_i + 2]), label_name.index(line_i['emotions'][idx_i + 3]),  label_name.index(first_emotion)]\n",
        "                else:\n",
        "                    line_emo = [label_name.index(emotion_i), label_name.index(line_i['emotions'][idx_i + 1]),\n",
        "                                label_name.index(line_i['emotions'][idx_i + 2]),\n",
        "                                label_name.index(line_i['emotions'][idx_i + 3]),\n",
        "                                label_name.index(line_i['emotions'][idx_i + 4])]\n",
        "\n",
        "                try:\n",
        "                    label_list_.append(int(trigger_i))\n",
        "\n",
        "                    train_list_.append(line_emo)\n",
        "\n",
        "\n",
        "                except Exception as e:\n",
        "                    pass\n",
        "\n",
        "\n",
        "\n",
        "    return np.array(train_list_), np.array(label_list_)\n",
        "\n",
        "\n",
        "def read_json_test(path):\n",
        "    train_list_ = []\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as file_read:\n",
        "        data_line = json.loads(file_read.read())\n",
        "\n",
        "        for idx, line_i in enumerate(data_line):\n",
        "\n",
        "            for idx_i, speaker_i, utterances_i, emotion_i in zip(range(len(line_i['speakers'])),\n",
        "                                                                 line_i['speakers'],\n",
        "                                                                 line_i['utterances'],\n",
        "                                                                 line_i['emotions']):\n",
        "                if idx_i == len(line_i['speakers']) - 1:\n",
        "                    line_emo = [label_name.index(emotion_i), label_name.index(first_emotion),\n",
        "                                label_name.index(first_emotion), label_name.index(first_emotion), label_name.index(first_emotion)]\n",
        "                elif idx_i == len(line_i['speakers']) - 2:\n",
        "                    line_emo = [label_name.index(emotion_i), label_name.index(line_i['emotions'][idx_i + 1]),\n",
        "                                label_name.index(first_emotion), label_name.index(first_emotion), label_name.index(first_emotion)]\n",
        "                elif idx_i == len(line_i['speakers']) - 3:\n",
        "                    line_emo = [label_name.index(emotion_i), label_name.index(line_i['emotions'][idx_i + 1]),\n",
        "                                label_name.index(line_i['emotions'][idx_i + 2]), label_name.index(first_emotion), label_name.index(first_emotion)]\n",
        "                elif idx_i == len(line_i['speakers']) - 4:\n",
        "                    line_emo = [label_name.index(emotion_i), label_name.index(line_i['emotions'][idx_i + 1]),\n",
        "                                label_name.index(line_i['emotions'][idx_i + 2]), label_name.index(line_i['emotions'][idx_i + 3]), label_name.index(first_emotion)]\n",
        "                else:\n",
        "                    line_emo = [label_name.index(emotion_i), label_name.index(line_i['emotions'][idx_i + 1]),\n",
        "                                label_name.index(line_i['emotions'][idx_i + 2]),\n",
        "                                label_name.index(line_i['emotions'][idx_i + 3]),\n",
        "                                label_name.index(line_i['emotions'][idx_i + 4])]\n",
        "\n",
        "                train_list_.append(line_emo)\n",
        "\n",
        "    return np.array(train_list_)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    label_name = ['-1', 'disgust', 'contempt', 'anger', 'neutral', 'joy', 'sadness', 'fear', 'surprise']\n",
        "\n",
        "    first_emotion = \"-1\"\n",
        "\n",
        "\n",
        "    X_train, y_train = read_json(train_path)\n",
        "\n",
        "    X_test = read_json_test(test_path)\n",
        "    f1_list=[]\n",
        "\n",
        "      # Define the Random Forest\n",
        "\n",
        "    rf_clf = RandomForestClassifier(n_estimators=19, random_state=42)\n",
        "\n",
        "    # Train the classifier\n",
        "    rf_clf.fit(X_train, y_train)\n",
        "\n",
        "    # Predict on the test set\n",
        "    y_pred = rf_clf.predict(X_test)\n",
        "\n",
        "\n",
        "    from sklearn.metrics import f1_score\n",
        "    import json\n",
        "\n",
        "    # Load the JSON data from the file\n",
        "    with open(test_labels, 'r') as file:\n",
        "        data = json.load(file)\n",
        "\n",
        "    # Initialize an empty list to store the labels\n",
        "    labels_list = []\n",
        "\n",
        "    # Iterate through each entry in the JSON data\n",
        "    for entry in data:\n",
        "        # Extract the labels from the current entry and convert them to float\n",
        "        labels_list.extend([float(label) for label in entry['labels']])\n",
        "\n",
        "\n",
        "    answer_list=[]\n",
        "    for pred_i in y_pred:\n",
        "\n",
        "        answer_list.append(pred_i)\n",
        "\n",
        "    f1 = f1_score(labels_list,answer_list)\n",
        "\n",
        "    # Print the F1 score\n",
        "    print(\"F1 Score:\", f1)\n",
        "    f1_list.append(f1)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GCVORz8NHIxi"
      },
      "source": [
        "SVC CLASSIFIER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gExj6RoGUVSg",
        "outputId": "18cac5f7-de63-4c72-e1d3-3de3e9a3da1c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "F1 Score: 0.7450980392156863\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import numpy as np\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "\n",
        "def read_json(path):\n",
        "    train_list_ = []\n",
        "    label_list_ = []\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as file_read:\n",
        "        data_line = json.loads(file_read.read())\n",
        "\n",
        "\n",
        "        for idx, line_i in enumerate(data_line):\n",
        "\n",
        "\n",
        "            for idx_i, speaker_i, utterances_i, emotion_i, trigger_i in zip(range(len(line_i['speakers'])),\n",
        "                                                                            line_i['speakers'],\n",
        "                                                                            line_i['utterances'],\n",
        "                                                                            line_i['emotions'], line_i['triggers']):\n",
        "                if idx_i == len(line_i['speakers']) - 1:\n",
        "                    line_emo = [label_name.index(emotion_i), label_name.index(first_emotion),\n",
        "                                label_name.index(first_emotion), label_name.index(first_emotion)]\n",
        "                elif idx_i == len(line_i['speakers']) - 2:\n",
        "                    line_emo = [label_name.index(emotion_i), label_name.index(line_i['emotions'][idx_i + 1]),\n",
        "                                label_name.index(first_emotion), label_name.index(first_emotion)]\n",
        "                elif idx_i == len(line_i['speakers']) - 3:\n",
        "                    line_emo = [label_name.index(emotion_i), label_name.index(line_i['emotions'][idx_i + 1]),\n",
        "                                label_name.index(line_i['emotions'][idx_i + 2]), label_name.index(first_emotion)]\n",
        "                else:\n",
        "                    line_emo = [label_name.index(emotion_i), label_name.index(line_i['emotions'][idx_i + 1]),\n",
        "                                label_name.index(line_i['emotions'][idx_i + 2]),\n",
        "                                label_name.index(line_i['emotions'][idx_i + 3])]\n",
        "\n",
        "                try:\n",
        "                    label_list_.append(int(trigger_i))\n",
        "\n",
        "                    train_list_.append(line_emo)\n",
        "\n",
        "\n",
        "                except Exception as e:\n",
        "                    pass\n",
        "\n",
        "\n",
        "\n",
        "    return np.array(train_list_), np.array(label_list_)\n",
        "\n",
        "\n",
        "def read_json_test(path):\n",
        "    train_list_ = []\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as file_read:\n",
        "        data_line = json.loads(file_read.read())\n",
        "\n",
        "        for idx, line_i in enumerate(data_line):\n",
        "\n",
        "            for idx_i, speaker_i, utterances_i, emotion_i in zip(range(len(line_i['speakers'])),\n",
        "                                                                 line_i['speakers'],\n",
        "                                                                 line_i['utterances'],\n",
        "                                                                 line_i['emotions']):\n",
        "                if idx_i == len(line_i['speakers']) - 1:\n",
        "                    line_emo = [label_name.index(emotion_i), label_name.index(first_emotion),\n",
        "                                label_name.index(first_emotion), label_name.index(first_emotion)]\n",
        "                elif idx_i == len(line_i['speakers']) - 2:\n",
        "                    line_emo = [label_name.index(emotion_i), label_name.index(line_i['emotions'][idx_i + 1]),\n",
        "                                label_name.index(first_emotion), label_name.index(first_emotion)]\n",
        "                elif idx_i == len(line_i['speakers']) - 3:\n",
        "                    line_emo = [label_name.index(emotion_i), label_name.index(line_i['emotions'][idx_i + 1]),\n",
        "                                label_name.index(line_i['emotions'][idx_i + 2]), label_name.index(first_emotion)]\n",
        "                else:\n",
        "                    line_emo = [label_name.index(emotion_i), label_name.index(line_i['emotions'][idx_i + 1]),\n",
        "                                label_name.index(line_i['emotions'][idx_i + 2]),\n",
        "                                label_name.index(line_i['emotions'][idx_i + 3])]\n",
        "\n",
        "                train_list_.append(line_emo)\n",
        "\n",
        "    return np.array(train_list_)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    label_name = ['-1', 'disgust', 'contempt', 'anger', 'neutral', 'joy', 'sadness', 'fear', 'surprise']\n",
        "\n",
        "    first_emotion = \"-1\"\n",
        "\n",
        "\n",
        "\n",
        "    X_train, y_train = read_json(train_path)\n",
        "\n",
        "    X_test = read_json_test(test_path)\n",
        "\n",
        "\n",
        "\n",
        "    svm_clf = SVC(kernel='linear', C=1, random_state=42)\n",
        "\n",
        "\n",
        "    svm_clf.fit(X_train, y_train)\n",
        "\n",
        "    y_pred = svm_clf.predict(X_test)\n",
        "\n",
        "    from sklearn.metrics import f1_score\n",
        "    import json\n",
        "\n",
        "    # Load the JSON data from the file\n",
        "    with open(test_labels, 'r') as file:\n",
        "        data = json.load(file)\n",
        "\n",
        "    # Initialize an empty list to store the labels\n",
        "    labels_list = []\n",
        "\n",
        "    # Iterate through each entry in the JSON data\n",
        "    for entry in data:\n",
        "        # Extract the labels from the current entry and convert them to float\n",
        "        labels_list.extend([float(label) for label in entry['labels']])\n",
        "\n",
        "\n",
        "    answer_list=[]\n",
        "    for pred_i in y_pred:\n",
        "\n",
        "        answer_list.append(pred_i)\n",
        "\n",
        "    f1 = f1_score(labels_list,answer_list)\n",
        "\n",
        "    # Print the F1 score\n",
        "    print(\"F1 Score:\", f1)\n",
        "    f1_list.append(f1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ofeZU0_CHN0X"
      },
      "source": [
        "LOGISTIC REGRESSION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N4CA6_p-bv7N",
        "outputId": "d6beb075-c0fb-433b-a620-f41a1ca6ffa3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "F1 Score: 0.6962552011095701\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "\n",
        "\n",
        "def read_json(path):\n",
        "    train_list_ = []\n",
        "    label_list_ = []\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as file_read:\n",
        "        data_line = json.loads(file_read.read())\n",
        "\n",
        "\n",
        "        for idx, line_i in enumerate(data_line):\n",
        "\n",
        "\n",
        "            for idx_i, speaker_i, utterances_i, emotion_i, trigger_i in zip(range(len(line_i['speakers'])),\n",
        "                                                                            line_i['speakers'],\n",
        "                                                                            line_i['utterances'],\n",
        "                                                                            line_i['emotions'], line_i['triggers']):\n",
        "                if idx_i == len(line_i['speakers']) - 1:\n",
        "                    line_emo = [label_name.index(emotion_i), label_name.index(first_emotion),\n",
        "                                label_name.index(first_emotion), label_name.index(first_emotion)]\n",
        "                elif idx_i == len(line_i['speakers']) - 2:\n",
        "                    line_emo = [label_name.index(emotion_i), label_name.index(line_i['emotions'][idx_i + 1]),\n",
        "                                label_name.index(first_emotion), label_name.index(first_emotion)]\n",
        "                elif idx_i == len(line_i['speakers']) - 3:\n",
        "                    line_emo = [label_name.index(emotion_i), label_name.index(line_i['emotions'][idx_i + 1]),\n",
        "                                label_name.index(line_i['emotions'][idx_i + 2]), label_name.index(first_emotion)]\n",
        "                else:\n",
        "                    line_emo = [label_name.index(emotion_i), label_name.index(line_i['emotions'][idx_i + 1]),\n",
        "                                label_name.index(line_i['emotions'][idx_i + 2]),\n",
        "                                label_name.index(line_i['emotions'][idx_i + 3])]\n",
        "\n",
        "                try:\n",
        "                    label_list_.append(int(trigger_i))\n",
        "\n",
        "                    train_list_.append(line_emo)\n",
        "\n",
        "\n",
        "                except Exception as e:\n",
        "                    pass\n",
        "\n",
        "\n",
        "\n",
        "    return np.array(train_list_), np.array(label_list_)\n",
        "\n",
        "\n",
        "def read_json_test(path):\n",
        "    train_list_ = []\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as file_read:\n",
        "        data_line = json.loads(file_read.read())\n",
        "\n",
        "        for idx, line_i in enumerate(data_line):\n",
        "\n",
        "            for idx_i, speaker_i, utterances_i, emotion_i in zip(range(len(line_i['speakers'])),\n",
        "                                                                 line_i['speakers'],\n",
        "                                                                 line_i['utterances'],\n",
        "                                                                 line_i['emotions']):\n",
        "                if idx_i == len(line_i['speakers']) - 1:\n",
        "                    line_emo = [label_name.index(emotion_i), label_name.index(first_emotion),\n",
        "                                label_name.index(first_emotion), label_name.index(first_emotion)]\n",
        "                elif idx_i == len(line_i['speakers']) - 2:\n",
        "                    line_emo = [label_name.index(emotion_i), label_name.index(line_i['emotions'][idx_i + 1]),\n",
        "                                label_name.index(first_emotion), label_name.index(first_emotion)]\n",
        "                elif idx_i == len(line_i['speakers']) - 3:\n",
        "                    line_emo = [label_name.index(emotion_i), label_name.index(line_i['emotions'][idx_i + 1]),\n",
        "                                label_name.index(line_i['emotions'][idx_i + 2]), label_name.index(first_emotion)]\n",
        "                else:\n",
        "                    line_emo = [label_name.index(emotion_i), label_name.index(line_i['emotions'][idx_i + 1]),\n",
        "                                label_name.index(line_i['emotions'][idx_i + 2]),\n",
        "                                label_name.index(line_i['emotions'][idx_i + 3])]\n",
        "\n",
        "                train_list_.append(line_emo)\n",
        "\n",
        "    return np.array(train_list_)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    label_name = ['-1', 'disgust', 'contempt', 'anger', 'neutral', 'joy', 'sadness', 'fear', 'surprise']\n",
        "\n",
        "    first_emotion = \"-1\"\n",
        "\n",
        "\n",
        "    X_train, y_train = read_json(train_path)\n",
        "\n",
        "    X_test = read_json_test(test_path)\n",
        "\n",
        "\n",
        "    LogisticRegressio = LogisticRegression()\n",
        "\n",
        "    LogisticRegressio.fit(X_train, y_train)\n",
        "\n",
        "    y_pred = LogisticRegressio.predict(X_test)\n",
        "\n",
        "    # Initialize an empty list to store the labels\n",
        "    labels_list = []\n",
        "\n",
        "    # Iterate through each entry in the JSON data\n",
        "    for entry in data:\n",
        "        # Extract the labels from the current entry and convert them to float\n",
        "        labels_list.extend([float(label) for label in entry['labels']])\n",
        "\n",
        "\n",
        "    answer_list=[]\n",
        "    for pred_i in y_pred:\n",
        "\n",
        "        answer_list.append(pred_i)\n",
        "\n",
        "    f1 = f1_score(labels_list,answer_list)\n",
        "\n",
        "    # Print the F1 score\n",
        "    print(\"F1 Score:\", f1)\n",
        "    f1_list.append(f1)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O1qjmowdHVRK"
      },
      "source": [
        "KNN CLASSIFIER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y80HYliAcvo0",
        "outputId": "028bd275-c612-425a-c444-93df251d2a8a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "F1 Score: 0.7783375314861462\n"
          ]
        }
      ],
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "import json\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def read_json(path):\n",
        "    train_list_ = []\n",
        "    label_list_ = []\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as file_read:\n",
        "        data_line = json.loads(file_read.read())\n",
        "\n",
        "\n",
        "        for idx, line_i in enumerate(data_line):\n",
        "\n",
        "\n",
        "            for idx_i, speaker_i, utterances_i, emotion_i, trigger_i in zip(range(len(line_i['speakers'])),\n",
        "                                                                            line_i['speakers'],\n",
        "                                                                            line_i['utterances'],\n",
        "                                                                            line_i['emotions'], line_i['triggers']):\n",
        "                if idx_i == len(line_i['speakers']) - 1:\n",
        "                    line_emo = [label_name.index(emotion_i), label_name.index(first_emotion),\n",
        "                                label_name.index(first_emotion), label_name.index(first_emotion)]\n",
        "                elif idx_i == len(line_i['speakers']) - 2:\n",
        "                    line_emo = [label_name.index(emotion_i), label_name.index(line_i['emotions'][idx_i + 1]),\n",
        "                                label_name.index(first_emotion), label_name.index(first_emotion)]\n",
        "                elif idx_i == len(line_i['speakers']) - 3:\n",
        "                    line_emo = [label_name.index(emotion_i), label_name.index(line_i['emotions'][idx_i + 1]),\n",
        "                                label_name.index(line_i['emotions'][idx_i + 2]), label_name.index(first_emotion)]\n",
        "                else:\n",
        "                    line_emo = [label_name.index(emotion_i), label_name.index(line_i['emotions'][idx_i + 1]),\n",
        "                                label_name.index(line_i['emotions'][idx_i + 2]),\n",
        "                                label_name.index(line_i['emotions'][idx_i + 3])]\n",
        "\n",
        "                try:\n",
        "                    label_list_.append(int(trigger_i))\n",
        "\n",
        "                    train_list_.append(line_emo)\n",
        "\n",
        "\n",
        "                except Exception as e:\n",
        "                    pass\n",
        "\n",
        "\n",
        "\n",
        "    return np.array(train_list_), np.array(label_list_)\n",
        "\n",
        "\n",
        "def read_json_test(path):\n",
        "    train_list_ = []\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as file_read:\n",
        "        data_line = json.loads(file_read.read())\n",
        "\n",
        "        for idx, line_i in enumerate(data_line):\n",
        "\n",
        "            for idx_i, speaker_i, utterances_i, emotion_i in zip(range(len(line_i['speakers'])),\n",
        "                                                                 line_i['speakers'],\n",
        "                                                                 line_i['utterances'],\n",
        "                                                                 line_i['emotions']):\n",
        "                if idx_i == len(line_i['speakers']) - 1:\n",
        "                    line_emo = [label_name.index(emotion_i), label_name.index(first_emotion),\n",
        "                                label_name.index(first_emotion), label_name.index(first_emotion)]\n",
        "                elif idx_i == len(line_i['speakers']) - 2:\n",
        "                    line_emo = [label_name.index(emotion_i), label_name.index(line_i['emotions'][idx_i + 1]),\n",
        "                                label_name.index(first_emotion), label_name.index(first_emotion)]\n",
        "                elif idx_i == len(line_i['speakers']) - 3:\n",
        "                    line_emo = [label_name.index(emotion_i), label_name.index(line_i['emotions'][idx_i + 1]),\n",
        "                                label_name.index(line_i['emotions'][idx_i + 2]), label_name.index(first_emotion)]\n",
        "                else:\n",
        "                    line_emo = [label_name.index(emotion_i), label_name.index(line_i['emotions'][idx_i + 1]),\n",
        "                                label_name.index(line_i['emotions'][idx_i + 2]),\n",
        "                                label_name.index(line_i['emotions'][idx_i + 3])]\n",
        "\n",
        "                train_list_.append(line_emo)\n",
        "\n",
        "    return np.array(train_list_)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    label_name = ['-1', 'disgust', 'contempt', 'anger', 'neutral', 'joy', 'sadness', 'fear', 'surprise']\n",
        "\n",
        "    first_emotion = \"-1\"\n",
        "\n",
        "\n",
        "    X_train, y_train = read_json(train_path)\n",
        "\n",
        "    X_test = read_json_test(test_path)\n",
        "\n",
        "      # Create a KNN classifier with k=5(optimal)\n",
        "    clf = KNeighborsClassifier(n_neighbors=5)\n",
        "\n",
        "    # Train the classifier\n",
        "    clf.fit(X_train, y_train)\n",
        "\n",
        "    # Make predictions\n",
        "    y_pred = clf.predict(X_test)\n",
        "\n",
        "\n",
        "    from sklearn.metrics import f1_score\n",
        "    import json\n",
        "\n",
        "    # Load the JSON data from the file\n",
        "    with open(test_labels, 'r') as file:\n",
        "        data = json.load(file)\n",
        "\n",
        "    # Initialize an empty list to store the labels\n",
        "    labels_list = []\n",
        "\n",
        "    # Iterate through each entry in the JSON data\n",
        "    for entry in data:\n",
        "        # Extract the labels from the current entry and convert them to float\n",
        "        labels_list.extend([float(label) for label in entry['labels']])\n",
        "\n",
        "\n",
        "    answer_list=[]\n",
        "    for pred_i in y_pred:\n",
        "\n",
        "        answer_list.append(pred_i)\n",
        "\n",
        "    f1 = f1_score(labels_list,answer_list)\n",
        "\n",
        "    # Print the F1 score\n",
        "    print(\"F1 Score:\", f1, )\n",
        "    f1_list.append(f1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yke4vBDtHaGT"
      },
      "source": [
        "GRADIENT BOOSTING CLASSIFIER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "okr86ngsdLnZ",
        "outputId": "2d4144ba-202c-45e3-8caf-d5a0d61e2d2b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "F1 Score: 0.7915106117353308\n"
          ]
        }
      ],
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "import json\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def read_json(path):\n",
        "    train_list_ = []\n",
        "    label_list_ = []\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as file_read:\n",
        "        data_line = json.loads(file_read.read())\n",
        "\n",
        "\n",
        "        for idx, line_i in enumerate(data_line):\n",
        "\n",
        "\n",
        "            for idx_i, speaker_i, utterances_i, emotion_i, trigger_i in zip(range(len(line_i['speakers'])),\n",
        "                                                                            line_i['speakers'],\n",
        "                                                                            line_i['utterances'],\n",
        "                                                                            line_i['emotions'], line_i['triggers']):\n",
        "                if idx_i == len(line_i['speakers']) - 1:\n",
        "                    line_emo = [label_name.index(emotion_i), label_name.index(first_emotion),\n",
        "                                label_name.index(first_emotion), label_name.index(first_emotion)]\n",
        "                elif idx_i == len(line_i['speakers']) - 2:\n",
        "                    line_emo = [label_name.index(emotion_i), label_name.index(line_i['emotions'][idx_i + 1]),\n",
        "                                label_name.index(first_emotion), label_name.index(first_emotion)]\n",
        "                elif idx_i == len(line_i['speakers']) - 3:\n",
        "                    line_emo = [label_name.index(emotion_i), label_name.index(line_i['emotions'][idx_i + 1]),\n",
        "                                label_name.index(line_i['emotions'][idx_i + 2]), label_name.index(first_emotion)]\n",
        "                else:\n",
        "                    line_emo = [label_name.index(emotion_i), label_name.index(line_i['emotions'][idx_i + 1]),\n",
        "                                label_name.index(line_i['emotions'][idx_i + 2]),\n",
        "                                label_name.index(line_i['emotions'][idx_i + 3])]\n",
        "\n",
        "                try:\n",
        "                    label_list_.append(int(trigger_i))\n",
        "\n",
        "                    train_list_.append(line_emo)\n",
        "\n",
        "\n",
        "                except Exception as e:\n",
        "                    pass\n",
        "\n",
        "\n",
        "\n",
        "    return np.array(train_list_), np.array(label_list_)\n",
        "\n",
        "\n",
        "def read_json_test(path):\n",
        "    train_list_ = []\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as file_read:\n",
        "        data_line = json.loads(file_read.read())\n",
        "\n",
        "        for idx, line_i in enumerate(data_line):\n",
        "\n",
        "            for idx_i, speaker_i, utterances_i, emotion_i in zip(range(len(line_i['speakers'])),\n",
        "                                                                 line_i['speakers'],\n",
        "                                                                 line_i['utterances'],\n",
        "                                                                 line_i['emotions']):\n",
        "                if idx_i == len(line_i['speakers']) - 1:\n",
        "                    line_emo = [label_name.index(emotion_i), label_name.index(first_emotion),\n",
        "                                label_name.index(first_emotion), label_name.index(first_emotion)]\n",
        "                elif idx_i == len(line_i['speakers']) - 2:\n",
        "                    line_emo = [label_name.index(emotion_i), label_name.index(line_i['emotions'][idx_i + 1]),\n",
        "                                label_name.index(first_emotion), label_name.index(first_emotion)]\n",
        "                elif idx_i == len(line_i['speakers']) - 3:\n",
        "                    line_emo = [label_name.index(emotion_i), label_name.index(line_i['emotions'][idx_i + 1]),\n",
        "                                label_name.index(line_i['emotions'][idx_i + 2]), label_name.index(first_emotion)]\n",
        "                else:\n",
        "                    line_emo = [label_name.index(emotion_i), label_name.index(line_i['emotions'][idx_i + 1]),\n",
        "                                label_name.index(line_i['emotions'][idx_i + 2]),\n",
        "                                label_name.index(line_i['emotions'][idx_i + 3])]\n",
        "\n",
        "                train_list_.append(line_emo)\n",
        "\n",
        "    return np.array(train_list_)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    label_name = ['-1', 'disgust', 'contempt', 'anger', 'neutral', 'joy', 'sadness', 'fear', 'surprise']\n",
        "\n",
        "    first_emotion = \"-1\"\n",
        "\n",
        "\n",
        "\n",
        "    X_train, y_train = read_json(train_path)\n",
        "\n",
        "    X_test = read_json_test(test_path)\n",
        "\n",
        "\n",
        "    # Create a gradient boosting classifier\n",
        "    clf = GradientBoostingClassifier()\n",
        "\n",
        "    # Train the classifier\n",
        "    clf.fit(X_train, y_train)\n",
        "\n",
        "    # Make predictions\n",
        "    y_pred = clf.predict(X_test)\n",
        "\n",
        "\n",
        "    # Initialize an empty list to store the labels\n",
        "    labels_list = []\n",
        "\n",
        "    # Iterate through each entry in the JSON data\n",
        "    for entry in data:\n",
        "        # Extract the labels from the current entry and convert them to float\n",
        "        labels_list.extend([float(label) for label in entry['labels']])\n",
        "\n",
        "\n",
        "    answer_list=[]\n",
        "    for pred_i in y_pred:\n",
        "\n",
        "        answer_list.append(pred_i)\n",
        "\n",
        "    f1 = f1_score(labels_list,answer_list)\n",
        "\n",
        "    # Print the F1 score\n",
        "    print(\"F1 Score:\", f1)\n",
        "    f1_list.append(f1)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BjyAp2IRHelG"
      },
      "source": [
        "NAIVE BAYES CLASSIFIER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ySx_rDldtBc",
        "outputId": "9607b588-0420-4bfb-e4b7-52c9d3db9ca2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "F1 Score: 0.6164154103852596\n"
          ]
        }
      ],
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "import json\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def read_json(path):\n",
        "    train_list_ = []\n",
        "    label_list_ = []\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as file_read:\n",
        "        data_line = json.loads(file_read.read())\n",
        "\n",
        "\n",
        "        for idx, line_i in enumerate(data_line):\n",
        "\n",
        "\n",
        "            for idx_i, speaker_i, utterances_i, emotion_i, trigger_i in zip(range(len(line_i['speakers'])),\n",
        "                                                                            line_i['speakers'],\n",
        "                                                                            line_i['utterances'],\n",
        "                                                                            line_i['emotions'], line_i['triggers']):\n",
        "                if idx_i == len(line_i['speakers']) - 1:\n",
        "                    line_emo = [label_name.index(emotion_i), label_name.index(first_emotion),\n",
        "                                label_name.index(first_emotion), label_name.index(first_emotion)]\n",
        "                elif idx_i == len(line_i['speakers']) - 2:\n",
        "                    line_emo = [label_name.index(emotion_i), label_name.index(line_i['emotions'][idx_i + 1]),\n",
        "                                label_name.index(first_emotion), label_name.index(first_emotion)]\n",
        "                elif idx_i == len(line_i['speakers']) - 3:\n",
        "                    line_emo = [label_name.index(emotion_i), label_name.index(line_i['emotions'][idx_i + 1]),\n",
        "                                label_name.index(line_i['emotions'][idx_i + 2]), label_name.index(first_emotion)]\n",
        "                else:\n",
        "                    line_emo = [label_name.index(emotion_i), label_name.index(line_i['emotions'][idx_i + 1]),\n",
        "                                label_name.index(line_i['emotions'][idx_i + 2]),\n",
        "                                label_name.index(line_i['emotions'][idx_i + 3])]\n",
        "\n",
        "                try:\n",
        "                    label_list_.append(int(trigger_i))\n",
        "\n",
        "                    train_list_.append(line_emo)\n",
        "\n",
        "\n",
        "                except Exception as e:\n",
        "                    pass\n",
        "\n",
        "\n",
        "\n",
        "    return np.array(train_list_), np.array(label_list_)\n",
        "\n",
        "\n",
        "def read_json_test(path):\n",
        "    train_list_ = []\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as file_read:\n",
        "        data_line = json.loads(file_read.read())\n",
        "\n",
        "        for idx, line_i in enumerate(data_line):\n",
        "\n",
        "            for idx_i, speaker_i, utterances_i, emotion_i in zip(range(len(line_i['speakers'])),\n",
        "                                                                 line_i['speakers'],\n",
        "                                                                 line_i['utterances'],\n",
        "                                                                 line_i['emotions']):\n",
        "                if idx_i == len(line_i['speakers']) - 1:\n",
        "                    line_emo = [label_name.index(emotion_i), label_name.index(first_emotion),\n",
        "                                label_name.index(first_emotion), label_name.index(first_emotion)]\n",
        "                elif idx_i == len(line_i['speakers']) - 2:\n",
        "                    line_emo = [label_name.index(emotion_i), label_name.index(line_i['emotions'][idx_i + 1]),\n",
        "                                label_name.index(first_emotion), label_name.index(first_emotion)]\n",
        "                elif idx_i == len(line_i['speakers']) - 3:\n",
        "                    line_emo = [label_name.index(emotion_i), label_name.index(line_i['emotions'][idx_i + 1]),\n",
        "                                label_name.index(line_i['emotions'][idx_i + 2]), label_name.index(first_emotion)]\n",
        "                else:\n",
        "                    line_emo = [label_name.index(emotion_i), label_name.index(line_i['emotions'][idx_i + 1]),\n",
        "                                label_name.index(line_i['emotions'][idx_i + 2]),\n",
        "                                label_name.index(line_i['emotions'][idx_i + 3])]\n",
        "\n",
        "                train_list_.append(line_emo)\n",
        "\n",
        "    return np.array(train_list_)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    label_name = ['-1', 'disgust', 'contempt', 'anger', 'neutral', 'joy', 'sadness', 'fear', 'surprise']\n",
        "\n",
        "    first_emotion = \"-1\"\n",
        "\n",
        "\n",
        "    X_train, y_train = read_json(train_path)\n",
        "\n",
        "    X_test = read_json_test(test_path)\n",
        "\n",
        "\n",
        "    # Create a Gaussian Naive Bayes classifier\n",
        "    clf = GaussianNB()\n",
        "\n",
        "    # Train the classifier\n",
        "    clf.fit(X_train, y_train)\n",
        "\n",
        "    # Make predictions\n",
        "    y_pred = clf.predict(X_test)\n",
        "\n",
        "\n",
        "    # Initialize an empty list to store the labels\n",
        "    labels_list = []\n",
        "\n",
        "    # Iterate through each entry in the JSON data\n",
        "    for entry in data:\n",
        "        # Extract the labels from the current entry and convert them to float\n",
        "        labels_list.extend([float(label) for label in entry['labels']])\n",
        "\n",
        "\n",
        "    answer_list=[]\n",
        "    for pred_i in y_pred:\n",
        "\n",
        "        answer_list.append(pred_i)\n",
        "\n",
        "    f1 = f1_score(labels_list,answer_list)\n",
        "\n",
        "    # Print the F1 score\n",
        "    print(\"F1 Score:\", f1)\n",
        "    f1_list.append(f1)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
